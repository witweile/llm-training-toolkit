import argparse\nimport os\n\nimport torch\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, set_peft_model_state_dict\nfrom torch.utils.data import IterableDataset\nfrom tqdm import tqdm\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, logging, set_seed\nfrom transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n\n\"\"\"\nFine-Tune StarCoder on Code Alpaca/SE\n\"\"\"\n\nclass SavePeftModelCallback(TrainerCallback):\n    def on_save(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n\n        kwargs[\"model\"].save_pretrained(checkpoint_folder)\n\n        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n        torch.save({}, pytorch_model_path)\n        return control\n\n\nclass LoadBestPeftModelCallback(TrainerCallback):\n    def on_train_end(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        print(f\"Loading best peft model from {state.best_model_checkpoint} (score: {state.best_metric}).\")\n        best_model_path = os.path.join(state.best_model_checkpoint, \"adapter_model.bin\")\n        adapters_weights = torch.load(best_model_path)\n        model = kwargs[\"model\"]\n        set_peft_model_state_dict(model, adapters_weights)\n        return control\n    \n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_path\", type=str, default=\"bigcode/large-model\")\n    parser.add_argument(\"--dataset_name\", type=str, default=\"HuggingFaceH4/CodeAlpaca_20K\")\n    parser.add_argument(\"--subset\", type=str)\n    parser.add_argument(\"--split\", type=str)\n    parser.add_argument(\"--size_valid_set\", type=int, default=10000)\n    parser.add_argument(\"--streaming\", action=\"store_true\")\n    parser.add_argument(\"--shuffle_buffer\", type=int, default=5000)\n\n    parser.add_argument(\"--input_column_name\", type=str, default=\"prompt\")\n    parser.add_argument(\"--output_column_name\", type=str, default=\"completion\")\n\n    parser.add_argument(\"--seq_length\", type=int, default=2048)\n    parser.add_argument(\"--max_steps\", type=int, default=10000)\n    parser.add_argument(\"--batch_size\", type=int, default=1)\n    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=16)\n    parser.add_argument(\"--eos_token_id\", type=int, default=49152)\n\n    parser.add_argument(\"--lora_r\", type=int, default=16)\n    parser.add_argument(\"--lora_alpha\", type=int, default=32)\n    parser.add_argument(\"--lora_dropout\", type=float, default=0.05)\n\n    parser.add_argument(\"--learning_rate\", type=float, default=5e-6)\n    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")\n    parser.add_argument(\"--num_warmup_steps\", type=int, default=100)\n    parser.add_argument(\"--weight_decay\", type=float, default=0.05)\n\n    parser.add_argument(\"--local_rank\", type=int, default=0)\n    parser.add_argument(\"--no_fp16\", action=\"store_false\")\n    parser.add_argument(\"--bf16\", action=\"store_true\", default=True)\n    parser.add_argument(\"--no_gradient_checkpointing\", action=\"store_false\", default=False)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--num_workers\", type=int, default=None)\n    parser.add_argument(\"--output_dir\", type=str, default=\"./checkpoints\")\n    parser.add_argument(\"--log_freq\", default=100, type=int)\n    parser.add_argument(\"--eval_freq\", default=100, type=int)\n    parser.add_argument(\"--save_freq\", default=1000, type=int)\n\n    return parser.parse_args()\n\n\ndef chars_token_ratio(dataset, tokenizer, input_column_name=\"prompt\", output_column_name=\"completion\", nb_examples=400):\n    \"\"\"\n    Estimate the average number of characters per token in the dataset.\n    \"\"\"\n    total_characters, total_tokens = 0, 0\n    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n        text = prepare_sample_text(example, input_column_name, output_column_name)\n        total_characters += len(text)\n        if tokenizer.is_fast:\n            total_tokens += len(tokenizer(text).tokens())\n        else:\n            total_tokens += len(tokenizer.tokenize(text))\n\n    return total_characters / total_tokens\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef prepare_sample_text(example, input_column_name=\"prompt\", output_column_name=\"completion\"):\n    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n    text = f\"Question: {example[input_column_name]}\\n\\nAnswer: {example[output_column_name]}\"\n    return text\n\n\nclass ConstantLengthDataset(IterableDataset):\n    \"\"\"\n    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n        Args:\n            tokenizer (Tokenizer): The processor used for proccessing the data.\n            dataset (dataset.Dataset): Dataset with text files.\n            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n            seq_length (int): Length of token sequences to return.\n            num_of_sequences (int): Number of token sequences to keep in buffer.\n            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenizer,\n        dataset,\n        infinite=False,\n        seq_length=1024,\n        num_of_sequences=1024,\n        chars_per_token=3.6,\n        input_column_name=\"prompt\",\n        output_column_name=\"completion\"\n    ):\n        self.tokenizer = tokenizer\n        self.concat_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else args.eos_token_id\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.infinite = infinite\n        self.current_size = 0\n        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n        self.input_column_name = input_column_name\n        self.output_column_name = output_column_name\n\n    def __iter__(self):\n        iterator = iter(self.dataset)\n        more_examples = True\n        while more_examples:\n            buffer, buffer_len = [], 0\n            while True:\n                if buffer_len >= self.max_buffer_size:\n                    break\n                try:\n                    buffer.append(prepare_sample_text(next(iterator), self.input_column_name, self.output_column_name))\n                    buffer_len += len(buffer[-1])\n                except StopIteration:\n                    if self.infinite:\n                        iterator = iter(self.dataset)\n                    else:\n                        more_examples = False\n                        break\n            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n            all_token_ids = []\n            for tokenized_input in tokenized_inputs:\n                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n            for i in range(0, len(all_token_ids), self.seq_length):\n                input_ids = all_token_ids[i : i + self.seq_length]\n                if len(input_ids) == self.seq_length:\n                    self.current_size += 1\n                    yield {\n                        \"input_ids\": torch.LongTensor(input_ids),\n                        \"labels\": torch.LongTensor(input_ids),\n                    }\n\n\ndef create_datasets(tokenizer, args):\n    dataset = load_dataset(\n        args.dataset_name,\n        data_dir=args.subset,\n        split=args.split,\n        use_auth_token=True,\n        num_proc=args.num_workers if not args.streaming else None,\n        streaming=args.streaming,\n    )\n    if args.streaming:\n        print(\"Loading the dataset in streaming mode\")\n        valid_data = dataset.take(args.size_valid_set)\n        train_data = dataset.skip(args.size_valid_set)\n        train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)\n    else:\n        train_data = dataset[\"train\"]\n        valid_data = dataset[\"test\"]\n        print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n\n    chars_per_token = chars_token_ratio(train_data, tokenizer, args.input_column_name, args.output_column_name)\n    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n\n    train_dataset = ConstantLengthDataset(\n        tokenizer,\n        train_data,\n        infinite=True,\n        seq_length=args.seq_length,\n        chars_per_token=chars_per_token,\n        input_column_name=args.input_column_name,\n        output_column_name=args.output_column_name\n    )\n    valid_dataset = ConstantLengthDataset(\n        tokenizer,\n        valid_data,\n        infinite=False,\n        seq_length=args.seq_length,\n        chars_per_token=chars_per_token,\n        input_column_name=args.input_column_name,\n        output_column_name=args.output_column_name\n    )\n    return train_dataset, valid_dataset\n\n\ndef run_training(args, train_data, val_data):\n    print(\"Loading the model\")\n    # disable caching mechanism when using gradient checkpointing\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_path,\n        use_auth_token=True,\n        use_cache=not args.no_gradient_checkpointing,\n        load_in_8bit=True,\n        device_map={\"\": Accelerator().process_index},\n    )\n    model = prepare_model_for_int8_training(model)\n\n    lora_config = LoraConfig(\n        r=args.lora_r,\n        lora_alpha=args.lora_alpha,\n        lora_dropout=args.lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules = [\"c_proj\", \"c_attn\", \"q_attn\"]\n    )\n\n    model = get_peft_model(model, lora_config)\n\n    print_trainable_parameters(model)\n\n    train_data.start_iteration = 0\n\n    print(\"Starting main loop\")\n\n    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        dataloader_drop_last=True,\n        evaluation_strategy=\"steps\",\n        save_strategy=\"steps\",\n        load_best_model_at_end=True,\n        max_steps=args.max_steps,\n        eval_steps=args.eval_freq,\n        save_steps=args.save_freq,\n        logging_steps=args.log_freq,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        learning_rate=args.learning_rate,\n        lr_scheduler_type=args.lr_scheduler_type,\n        warmup_steps=args.num_warmup_steps,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        gradient_checkpointing=not args.no_gradient_checkpointing,\n        fp16=not args.no_fp16,\n        bf16=args.bf16,\n        weight_decay=args.weight_decay,\n        run_name=\"StarCoder-finetuned\",\n        report_to=\"wandb\",\n        ddp_find_unused_parameters=False,\n    )\n\n    trainer = Trainer(model=model, args=training_args, train_dataset=train_data, eval_dataset=val_data, callbacks=[SavePeftModelCallback, LoadBestPeftModelCallback])\n\n    print(\"Training...\")\n    trainer.train()\n\n    print(\"Saving last checkpoint of the model\")\n    model.save_pretrained(os.path.join(args.output_dir, \"final_checkpoint/\"))\n\n\ndef main(args):\n    tokenizer = AutoTokenizer.from_pretrained(args.model_path, use_auth_token=True)\n    train_dataset, eval_dataset = create_datasets(tokenizer, args)\n    run_training(args, train_dataset, eval_dataset)\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n\n    set_seed(args.seed)\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    logging.set_verbosity_error()\n\n    main(args)\n